Depth First Search (DFS):
Application: Web crawlers use DFS to explore web pages systematically, following links and indexing content for search engines. Write a simple program to index web pages using Depth First Search (DFS). The program should simulate a web graph where pages are represented as nodes and hyperlinks as edges.
PROGRAM :
#include <iostream>
using namespace std;
#define MAX 10   
void DFS(int graph[MAX][MAX], int visited[MAX], int start, int n) {
    cout << "Indexing page " << start << endl; 
    visited[start] = 1;    
    for (int i = 0; i < n; i++) {
        if (graph[start][i] == 1 && visited[i] == 0) {
            DFS(graph, visited, i, n);
        }
    }
}
int main() {
    int n;
    cout << "Enter number of web pages: ";
    cin >> n;
    int graph[MAX][MAX];
    int visited[MAX];
       for (int i = 0; i < n; i++)
 visited[i] = 0;
    cout << "\nEnter adjacency matrix (use 1 if link exists, else 0):\n";
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < n; j++) {
            cin >> graph[i][j];
        }
    }
    int start;
    cout << "\nEnter starting page number (0 to " << n-1 << "): ";
    cin >> start;
    cout << "\nStarting Web Crawling using DFS...\n";
    DFS(graph, visited, start, n);
    cout << "\nAll reachable pages have been indexed!\n";
    return 0;
}
